\section{Future Work}
In this section we will cover some features we deliberately left for future
work, since their implementation would have costed too much time for us to
integrate it.
Nevertheless, we tried to sketch blueprints of possible solutions for these
problems, in order to show they would actually be feasible.

\subsection{Overcoming worker threads greediness}
In our system, an executor loads work items on a single queue, which is
concurrently consumed by worker threads that fetch items from its head.
Clearly, if we increased the available cores and the number of worker threads,
this work queue could become a bottleneck since each worker thread tries to
extract a work item from the queue head as soon as there is one.

One possible way to tackle this problem would be to have more queues, one for
each worker thread. In this way, an executor may be able to choose among
different policies (round robin, fair-weighted queues, ...) to populate the
work item queues.

\subsection{More flexible topologies}
We generated the configuration files for the city instances with a custom
Python simulator. We built this tool in order to be able to produce large
instances without having to do it manually, as well as to have a higher degree
of variability from run to run (if we run several instances).

Even if we can tune several parameters in our generator, it is limited to a
certain set of topologies: for example, we assume to have a street composed by
a sidewalk, a bikeway, a roadway and again a bikeway and a sidewalk.
This hinders clearly the flexibility of our simulator, but it eased our work
when we had to verify the correctness of the system.

\subsection{Uber-like services}\label{sec:future-uber}
The application layer presents two types of RPC:

\begin{enumerate}
  \item a traveller moving from a district to an adjacent one;
  \item a traveller booking a parking spot.
\end{enumerate}

In the second case, the message is intended to be sent to a possibly
non-adjacent node. Therefore, the middleware has a system of naming and routing
capable of forwarding the message to the more suitable next-hop node.

Uber-like services are on-demand transport services offered by (1) normal
travellers which are driving a car or (2) taxis. In both cases, a traveller
which \textit{requests} a Uber-like service has to reach a mobile entity.
This factor would have made our system more complex in one of two possible
ways:

\begin{enumerate}
  \item if the Uber-like provider is specified by who is making the request,
    then we would have to track active entities at middleware layer (at the
    moment we don't have the need to, so we don't do it)
  \item if the Uber-like request does not specify an intended provider, then it
    should be propagated until it finds an available provider, if any.
\end{enumerate}

The first scenario would have led us to a more complex system of name
resolution, whereas the latter would have caused us to add to the middleware
interface the possibility to perform a RPC which spreads (possibly) all over
the system until satisfied.

\subsection{Emergency services}
Similarly to the previous point (section \ref{sec:future-uber}), we could add
another service to our application layer, i.e., emergency services like
ambulances.

This feature would be easier than Uber-like services to implement, since for a
traveller in a state of emergency this operation would be analogous to book a
parking spot (obviously with a different semantic after the booking operation).

\subsection{Fault tolerance -- Middleware}
The middleware is fault tolerant in the sense it is resilient against failures
during the boot phase, when sending messages to the application layer, and in
other scenarios.

However, in some cases a failure of the middleware may cause impediment in the
normal flow of the system: for example, if the process which is currently
supervising a snapshot dies, then when it is respawned there are chances it
never ends the snapshot.
This means that in this and other situations the system might suffer of a low
degree of fault tolerance. However, this lack of resiliency could be fixed by
addressing each possible problem with a suitable patch: for example, snapshot
daemons may send snapshot markers again after a while if no response is
arriving from a given neighbor.

\subsection{Hierarchical naming}
We initially started the project by assigning integer identifiers to both
active and reactive entities. As we moved forward in the project, we leveraged
Ada's strong type system by performing a refactoring and defining two different
types for active and reactive identifiers, namely \texttt{Agent.Agent\_Id} and
\texttt{Infra\_Id}.

While for the former we chose to use strings (more precisely, Ada's Unbounded
Strings) to encode the identifier, for the latter we decided to keep using
integer numbers.

This decision was not so wise a posteriori, since we were not able to implement
a hierarchical name system for reactive entities, like \texttt{x.y.w.z}, where
\texttt{x} refers to the street, \texttt{y} to the way, \texttt{w} to the lane
and \texttt{z} to the specific stretch.

\subsection{Dynamic routing}
Our middleware currently bases its routing mechanism on static forwarding
tables. This is surely not a flexible choice, but we chose to do it so for the
sake of simplicity.

An alternative feasible solution could be to perform dynamic lookup for the
route when the message has to be sent in the first place by using some dynamic
routing protocol (e.g., AODV).

\subsection{Consistency of local snapshots}
The consistency of application layer snapshot is not currently guaranteed,
since they might contain duplicates or some travellers might disappear.

Clearly, a local snapshot is not a consistent checkpoint to resume the system
in case of failures. Therefore, we could also have added the following
features:

\begin{itemize}
  \item Snapshot Validator: a daemon which checks the integrity of an
        application-layer snapshot. If this snapshot is not valid, then it may
        act according to different policies, for example requiring another
        snapshot or simply dropping the invalid one;
  \item Snapshot Corrector: a daemon which works after a validation check for
        an application-layer snapshot fails. This component could try to fix
        the snapshot so it becomes feasible. E.g., if no traveller disappeared
        and the snapshot is invalid due to duplicates, it may simply
        arbitrarily drop some travellers from some stretches and make it valid,
        avoiding subsequent requests for valid snapshots;
  \item Resumption from a valid epoch: if we had the chance to guarantee a
        snapshot it's valid, we could make the system restart from a valid,
        distributed snapshot in case of failures. Clearly, this procedure would
        also stop nodes which are running without problems, just to make them
        restart executing from the last valid point in time.
        We also think that this procedure could be used to minimize clock
        drift across several nodes, though it may not be a the best solution
        to do it.
\end{itemize}

\subsection{Election protocol}
Even if we design the election protocol for our system, we did not implement it
because we did not need it for our system to work in the way we require: in
fact, we believe a simulation makes sense if all of its nodes are running (and
therefore the coordinator is always one specific node).

However, in some cases it could be useful to have a way to make the system
perform distributed operations gracefully even if some nodes are not working.
In those situations, an election protocol like the one described in Section
\ref{sec:election} would definitely be useful.

\subsection{Test coverage}
Even if we tested many of the main components of our system, we may want to
raise the test coverage for the backend.

Some backend packages are not tested at all, but their logic is very simple.

This is true for the middleware too (even if this layer has been thorougly
tested), in which some tests may gain accuracy by refactoring the code and
defining the dependencies in the configuration files for \textit{all} the
modules (as described in this
\href{http://blog.plataformatec.com.br/2015/10/mocks-and-explicit-contracts/}{blog post} by Jos\'e Valim).

\subsection{Code documentation}
We strove ourselves to make our code easily understandable, also because we
were working on a small-sized team.
Therefore, in absence of comments, we tried to make the flow of the code itself
self-explanatory; on the other hand, we pointed out complex pieces of code and
add explanation to them in the cases we thought it was not enough clear as it
was.

Always in this direction, we provided ourselves READMEs and explanations on how
to build and run our (sub)system(s). Also, the middleware source code is
accompanied by small guides to set up the configuration variables for the
different environments (\texttt{dev}, \texttt{test} and \texttt{prod}).

However:
\begin{itemize}
  \item middleware modules do not present a per-module documentation;
  \item middleware modules do not contain CLI\footnote{more precisely
    \textbf{iex}, that is Elixir's main REPL} examples on how to use them;
  \item only a fraction of backend packages have a detailed header. We decided
    to omit it in the case the header was incomplete;
  \item since the OOP model of Ada is quite distant from the standard ones and
    Elixir does not have any\footnote{actually there are mixins and
    interfaces, but we did not use them}, we used a short number of class
    diagrams to document our architecture.
\end{itemize}

\subsection{Shared references}
We implemented (but not used, except for seldom exceptions) a system of shared
references.
Shared references are basically smart pointers, i.e., augmented pointers which
keep count of the variables which are using them: in this way, if no variable
is no longer referencing a heap region, it can be freed without any risky.

Features like shared references are surely handy in the Ada runtime, since it
has no garbage collection. None of the implementations we found online were
actually working, so we implemented ourselves a custom one for non-limited
types.

Unfortunately, we came up with this idea too late and a refactoring would have
been too much burdensome: we just integrated shared references for a couple of
types to evaluate this refactoring, but we eventually gave up to carry on this
work for the whole application layer.
In fact, other than being onerous, we would have needed to implement a non
limited version of shared references (so they can be used along with some
Ada.Containers) and to implement shared references for limited types.

\subsection{Clock synchronization}
We did not make any effort to synchronize the clocks of different nodes.
In the case we did it (maybe sending absolute or relative timestamps to have 
some kind of alignment), we might have experienced some improvements in
time-dependent operations.

\subsection{Middleware communication semantics}
Thanks to RabbitMQ's persistent queues, we managed to have RR1
(\textit{Request Retry}) as our communication semantics at middleware layer.

We did implement neither DF (\textit{Duplicate Filter}) nor RR2
(\textit{Request Retransmit}), simply because we implemented the middleware in
a way we don't need them (it is safe against duplicates and idempotent
computations).

\subsection{High-level automation tool}
Most of our system is built and run by means of scripts which have a very
low-level feel. We could have gone instead for sophisticated build automation
systems like Chef or Puppet, but we deliberately did not because they would
have introduced further dependencies on the user side.

\subsection{Own frontend stack}
We decided to use the PEAP (Phoenix-Elixir-Angular-Postgres) stack for the
frontend, which we found online in a GitHub repository.

Even if it suited our needs, it resulted clumsy when it came to include PEAP
in the Docker build process: in fact it has external dependencies both towards
Elixir and NodeJS (which recursively called each other to resolve
Javascript dependencies and start the whole system).

Therefore, we were not able to decouple these two components with little
effort: instead, we think that it would probably be simpler if we built
ourselves the frontend stack in order to have more control on how to separate
the different parts in several Docker containers.
